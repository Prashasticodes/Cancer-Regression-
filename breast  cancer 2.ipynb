{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Prashasticodes/Cancer-Regression-/blob/main/breast%20%20cancer%202.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "\n",
        "# Step 1: Load the dataset\n",
        "cancer_data = load_breast_cancer()\n",
        "data = pd.DataFrame(data=cancer_data.data, columns=cancer_data.feature_names)\n",
        "\n",
        "# Note: The dataset does not contain a direct regression target, so for demonstration, we can create a target\n",
        "# For example, we could use the mean radius as a target for a regression task\n",
        "# Here, we assume that we're predicting the mean radius as a continuous target variable\n",
        "data['target'] = cancer_data.target  # This is typically a classification target (0 or 1)\n",
        "\n",
        "# Step 2: Feature selection\n",
        "features = data.columns[:-1]  # All columns except the target\n",
        "X = data[features]  # Features\n",
        "y = data['target']  # Target variable\n",
        "\n",
        "# Step 3: Data splitting\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Step 4: Model training\n",
        "model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Step 5: Predictions\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Step 6: Model evaluation\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "\n",
        "# Output evaluation metrics\n",
        "print(f'Mean Squared Error: {mse}')\n",
        "print(f'R-squared: {r2}')\n",
        "\n",
        "# Calculate accuracy percentage\n",
        "accuracy_percentage = r2 * 100\n",
        "print(f'Accuracy Percentage: {accuracy_percentage:.2f}%')\n",
        "\n",
        "# Optional: Feature Importance\n",
        "importances = model.feature_importances_\n",
        "feature_importance_df = pd.DataFrame({'feature': features, 'importance': importances})\n",
        "print(\"\\nFeature Importances:\")\n",
        "print(feature_importance_df.sort_values(by='importance', ascending=False))\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "THasXkjUd63q",
        "outputId": "2f8b16c0-e21e-4870-9598-c37f2f72459c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean Squared Error: 0.03276140350877193\n",
            "R-squared: 0.8605413691451032\n",
            "Accuracy Percentage: 86.05%\n",
            "\n",
            "Feature Importances:\n",
            "                    feature  importance\n",
            "27     worst concave points    0.269936\n",
            "7       mean concave points    0.202781\n",
            "22          worst perimeter    0.158947\n",
            "23               worst area    0.118171\n",
            "20             worst radius    0.103470\n",
            "21            worst texture    0.025475\n",
            "1              mean texture    0.021067\n",
            "26          worst concavity    0.011771\n",
            "13               area error    0.010239\n",
            "24         worst smoothness    0.008456\n",
            "4           mean smoothness    0.006652\n",
            "16          concavity error    0.006587\n",
            "19  fractal dimension error    0.005278\n",
            "3                 mean area    0.005143\n",
            "6            mean concavity    0.004656\n",
            "28           worst symmetry    0.004215\n",
            "12          perimeter error    0.003913\n",
            "11            texture error    0.003764\n",
            "10             radius error    0.003696\n",
            "18           symmetry error    0.003541\n",
            "14         smoothness error    0.003475\n",
            "17     concave points error    0.003262\n",
            "8             mean symmetry    0.002668\n",
            "2            mean perimeter    0.002537\n",
            "29  worst fractal dimension    0.002431\n",
            "9    mean fractal dimension    0.002150\n",
            "25        worst compactness    0.001836\n",
            "15        compactness error    0.001707\n",
            "0               mean radius    0.001405\n",
            "5          mean compactness    0.000774\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
        "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
        "from xgboost import XGBRegressor\n",
        "from lightgbm import LGBMRegressor\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "# Step 1: Load the dataset\n",
        "cancer_data = load_breast_cancer()\n",
        "data = pd.DataFrame(data=cancer_data.data, columns=cancer_data.feature_names)\n",
        "\n",
        "# Define a continuous target (using 'mean radius' as an example)\n",
        "data['target'] = data['mean radius']\n",
        "\n",
        "# Step 2: Feature selection (removing the current target column)\n",
        "features = data.columns[:-1]\n",
        "X = data[features]\n",
        "y = data['target']\n",
        "\n",
        "# Step 3: Feature scaling\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# Optional: Dimensionality reduction using PCA\n",
        "pca = PCA(n_components=10)  # Reduce to 10 components\n",
        "X_reduced = pca.fit_transform(X_scaled)\n",
        "\n",
        "# Step 4: Data splitting\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_reduced, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Function to evaluate models\n",
        "def evaluate_model(model, X_train, X_test, y_train, y_test):\n",
        "    model.fit(X_train, y_train)\n",
        "    y_pred = model.predict(X_test)\n",
        "    mse = mean_squared_error(y_test, y_pred)\n",
        "    r2 = r2_score(y_test, y_pred)\n",
        "    print(f'{model.__class__.__name__}:')\n",
        "    print(f'  Mean Squared Error: {mse:.4f}')\n",
        "    print(f'  R-squared: {r2:.4f}')\n",
        "    print(f'  Accuracy Percentage: {r2 * 100:.2f}%\\n')\n",
        "    return mse, r2\n",
        "\n",
        "# Step 5: Model Training and Evaluation\n",
        "# Random Forest\n",
        "rf_model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
        "rf_mse, rf_r2 = evaluate_model(rf_model, X_train, X_test, y_train, y_test)\n",
        "\n",
        "# Gradient Boosting\n",
        "gb_model = GradientBoostingRegressor(n_estimators=100, random_state=42)\n",
        "gb_mse, gb_r2 = evaluate_model(gb_model, X_train, X_test, y_train, y_test)\n",
        "\n",
        "# XGBoost\n",
        "xgb_model = XGBRegressor(n_estimators=100, random_state=42, verbosity=0)\n",
        "xgb_mse, xgb_r2 = evaluate_model(xgb_model, X_train, X_test, y_train, y_test)\n",
        "\n",
        "# LightGBM\n",
        "lgbm_model = LGBMRegressor(n_estimators=100, random_state=42)\n",
        "lgbm_mse, lgbm_r2 = evaluate_model(lgbm_model, X_train, X_test, y_train, y_test)\n",
        "\n",
        "# Step 6: Hyperparameter tuning (Example for Random Forest)\n",
        "param_grid_rf = {\n",
        "    'n_estimators': [50, 100, 200],\n",
        "    'max_depth': [None, 10, 20],\n",
        "    'min_samples_split': [2, 5, 10],\n",
        "    'min_samples_leaf': [1, 2, 4]\n",
        "}\n",
        "grid_search_rf = GridSearchCV(estimator=RandomForestRegressor(random_state=42),\n",
        "                              param_grid=param_grid_rf,\n",
        "                              cv=5,\n",
        "                              scoring='r2',\n",
        "                              n_jobs=-1)\n",
        "grid_search_rf.fit(X_train, y_train)\n",
        "best_rf_model = grid_search_rf.best_estimator_\n",
        "\n",
        "# Evaluate the best RF model\n",
        "print(\"Best Random Forest Model after GridSearchCV:\")\n",
        "rf_mse, rf_r2 = evaluate_model(best_rf_model, X_train, X_test, y_train, y_test)\n",
        "\n",
        "# Optional: Feature Importances from PCA Components (for models supporting it)\n",
        "importances = best_rf_model.feature_importances_\n",
        "print(\"Feature Importances (Reduced Dimensions):\")\n",
        "for i, imp in enumerate(importances):\n",
        "    print(f'PCA Component {i + 1}: {imp:.4f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "24tL5FAr6z1r",
        "outputId": "990306a8-90ec-4840-d82e-f0f7626a57f1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/dask/dataframe/__init__.py:42: FutureWarning: \n",
            "Dask dataframe query planning is disabled because dask-expr is not installed.\n",
            "\n",
            "You can install it with `pip install dask[dataframe]` or `conda install dask`.\n",
            "This will raise in a future version.\n",
            "\n",
            "  warnings.warn(msg, FutureWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RandomForestRegressor:\n",
            "  Mean Squared Error: 0.6048\n",
            "  R-squared: 0.9500\n",
            "  Accuracy Percentage: 95.00%\n",
            "\n",
            "GradientBoostingRegressor:\n",
            "  Mean Squared Error: 0.3326\n",
            "  R-squared: 0.9725\n",
            "  Accuracy Percentage: 97.25%\n",
            "\n",
            "XGBRegressor:\n",
            "  Mean Squared Error: 0.6806\n",
            "  R-squared: 0.9437\n",
            "  Accuracy Percentage: 94.37%\n",
            "\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000458 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 1520\n",
            "[LightGBM] [Info] Number of data points in the train set: 455, number of used features: 10\n",
            "[LightGBM] [Info] Start training from score 14.117635\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "LGBMRegressor:\n",
            "  Mean Squared Error: 0.7408\n",
            "  R-squared: 0.9387\n",
            "  Accuracy Percentage: 93.87%\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from sklearn.ensemble import StackingRegressor\n",
        "\n",
        "# Visualize Feature Importances\n",
        "def plot_feature_importances(importances, title):\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    indices = np.argsort(importances)[::-1]\n",
        "    plt.bar(range(len(importances)), importances[indices], align=\"center\")\n",
        "    plt.title(title)\n",
        "    plt.xlabel(\"Feature Index (PCA Component)\")\n",
        "    plt.ylabel(\"Importance Score\")\n",
        "    plt.show()\n",
        "\n",
        "# Plot feature importances for the best Random Forest model\n",
        "print(\"\\nVisualizing Feature Importances (Random Forest):\")\n",
        "plot_feature_importances(best_rf_model.feature_importances_, \"Random Forest Feature Importances (PCA Components)\")\n",
        "\n",
        "# Residual Plot Function\n",
        "def plot_residuals(y_test, y_pred, title):\n",
        "    residuals = y_test - y_pred\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    plt.scatter(y_pred, residuals, alpha=0.7)\n",
        "    plt.axhline(y=0, color='r', linestyle='--')\n",
        "    plt.title(title)\n",
        "    plt.xlabel(\"Predicted Values\")\n",
        "    plt.ylabel(\"Residuals\")\n",
        "    plt.show()\n",
        "\n",
        "# Plot residuals for the best Random Forest model\n",
        "print(\"\\nResidual Plot (Random Forest):\")\n",
        "plot_residuals(y_test, best_rf_model.predict(X_test), \"Residual Plot (Random Forest)\")\n",
        "\n",
        "# Stacking Regressor: Combine Multiple Models\n",
        "estimators = [\n",
        "    ('rf', RandomForestRegressor(n_estimators=100, random_state=42)),\n",
        "    ('gb', GradientBoostingRegressor(n_estimators=100, random_state=42)),\n",
        "    ('xgb', XGBRegressor(n_estimators=100, random_state=42, verbosity=0)),\n",
        "    ('lgbm', LGBMRegressor(n_estimators=100, random_state=42))\n",
        "]\n",
        "stacking_model = StackingRegressor(\n",
        "    estimators=estimators,\n",
        "    final_estimator=GradientBoostingRegressor(n_estimators=200, random_state=42)\n",
        ")\n",
        "\n",
        "# Train Stacking Model\n",
        "stacking_model.fit(X_train, y_train)\n",
        "y_pred_stacking = stacking_model.predict(X_test)\n",
        "\n",
        "# Evaluate Stacking Model\n",
        "print(\"\\nEvaluating Stacking Regressor:\")\n",
        "stacking_mse = mean_squared_error(y_test, y_pred_stacking)\n",
        "stacking_r2 = r2_score(y_test, y_pred_stacking)\n",
        "print(f'  Mean Squared Error: {stacking_mse:.4f}')\n",
        "print(f'  R-squared: {stacking_r2:.4f}')\n",
        "print(f'  Accuracy Percentage: {stacking_r2 * 100:.2f}%')\n",
        "\n",
        "# Residual Plot for Stacking Regressor\n",
        "print(\"\\nResidual Plot (Stacking Regressor):\")\n",
        "plot_residuals(y_test, y_pred_stacking, \"Residual Plot (Stacking Regressor)\")\n"
      ],
      "metadata": {
        "id": "kQQCsfUV738a"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "authorship_tag": "ABX9TyPmsFNuDdVyuKln4L8pZ+87",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}